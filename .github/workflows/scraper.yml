name: Daily Scraper

on:
  schedule:
    - cron: "0 * * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Check out repo
        uses: actions/checkout@v6
      - name: Set up Python
        uses: actions/setup-python@v5
      - uses: browser-actions/setup-chrome@v2
      - name: Run scraper
        working-directory: ./scraper
        run: |
          pip install -r requirements.txt
          python main.py

      - name: Set Git config
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      - name: Check for changes
        id: changes
        run: |
          git add .
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Create branch, commit, and open PR
        if: steps.changes.outputs.has_changes == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_TOKEN }}
        run: |
          BRANCH_NAME="scraper/update-${{ github.run_id }}-${{ github.run_attempt }}"
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
          git checkout -b "$BRANCH_NAME"
          git commit -m "Scraper results"
          git push origin "$BRANCH_NAME"
          gh pr create \
            --base main \
            --title "Scraper results ($TIMESTAMP)" \
            --body "Automated update from scraper workflow run #${{ github.run_id }}" \
            --label "automerge" \
            --label "scraper"
